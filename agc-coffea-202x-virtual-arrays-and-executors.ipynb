{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688000d2-0ab3-4ff8-a4cc-f112422bac42",
   "metadata": {},
   "source": [
    "# AGC + calver coffea on coffea-casa: CMS Open Data $t\\bar{t}$\n",
    "\n",
    "We'll base this on a few sources:\n",
    "- https://github.com/iris-hep/analysis-grand-challenge/tree/main/analyses/cms-open-data-ttbar (AGC, of course)\n",
    "- https://github.com/alexander-held/CompHEP-2023-AGC (contains a simplified version of AGC)\n",
    "- https://github.com/nsmith-/TTGamma_LongExercise/ (credit Nick Smith for helpful examples of the new API)\n",
    "- (and if time allows, weight features: https://github.com/CoffeaTeam/coffea/blob/backports-v0.7.x/binder/accumulators.ipynb / https://coffeateam.github.io/coffea/api/coffea.analysis_tools.Weights.html#coffea.analysis_tools.Weights.partial_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c03fef-22dc-4b52-8b4f-9583f423c59e",
   "metadata": {},
   "source": [
    "We are using [2015 CMS Open Data](https://cms.cern/news/first-cms-open-data-lhc-run-2-released) in this demonstration to showcase an analysis pipeline.\n",
    "This is a **technical demonstration**. We are including the relevant workflow aspects that physicists need in their work, but we are not focusing on making every piece of the demonstration physically meaningful. This concerns in particular systematic uncertainties: we capture the workflow, but the actual implementations are more complex in practice. If you are interested in the physics side of analyzing top pair production, check out the latest results from [ATLAS](https://twiki.cern.ch/twiki/bin/view/AtlasPublic/TopPublicResults) and [CMS](https://cms-results.web.cern.ch/cms-results/public-results/preliminary-results/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8537f4d-46b7-4f32-9c78-a3ed9705597d",
   "metadata": {},
   "source": [
    "This notebook implements most of the analysis pipeline shown in the following picture, using the tools also mentioned there:\n",
    "![ecosystem visualization](figures/pipe.001.jpeg)\n",
    "\n",
    "This version also includes the Combine tool to perform model building ans statistical inference. Despite not strictly being part of the ecosystem, it is widely used in the CMS collaboration, so in this tutorial we will see an example of statistical inference performed with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1984b-46df-4124-93f9-b5dc41848db5",
   "metadata": {},
   "source": [
    "## Imports: setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f4d32-e5c4-42f9-84e8-430034493a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import hist\n",
    "import coffea\n",
    "import numpy as np\n",
    "import uproot\n",
    "from dask.distributed import Client\n",
    "import cloudpickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea.processor import ProcessorABC, Runner, DaskExecutor\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "import correctionlib\n",
    "\n",
    "import warnings\n",
    "\n",
    "import utils\n",
    "from utils.systematics import rand_gauss\n",
    "utils.plotting.set_style()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "\n",
    "import getpass\n",
    "whoami_output = getpass.getuser()\n",
    "if whoami_output == \"cms-jovyan\":\n",
    "    client = Client(\"tls://localhost:8786\")\n",
    "else:\n",
    "    from dask.distributed import LocalCluster\n",
    "    cluster = LocalCluster(n_workers=1, threads_per_worker=1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "print(f\"awkward: {ak.__version__}\")\n",
    "print(f\"uproot: {uproot.__version__}\")\n",
    "print(f\"hist: {hist.__version__}\")\n",
    "print(f\"coffea: {coffea.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b8cc4-ee4a-4294-86d8-f5db8cf15aaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Produce an AGC histogram with plain awkward (no coffea yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667cbdf-9e6f-4c7b-8827-2f6bb35c670b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_trijet_mass(events):\n",
    "    # pT > 30 GeV for leptons, > 25 GeV for jets\n",
    "    selected_electrons = events.Electron[(events.Electron.pt > 30) & (np.abs(events.Electron.eta) < 2.1)]\n",
    "    selected_muons = events.Muon[(events.Muon.pt > 30) & (np.abs(events.Muon.eta) < 2.1)]\n",
    "    selected_jets = events.Jet[(events.Jet.pt > 25) & (np.abs(events.Jet.eta) < 2.4)]\n",
    "\n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # at least four jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "    B_TAG_THRESHOLD = 0.5\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "\n",
    "    # apply filters\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "\n",
    "    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    return ak.flatten(trijet_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da366d3-1480-4582-9f01-70a03e531da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttbar_file = \"https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/\"\\\n",
    "    \"TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_\"\\\n",
    "    \"mcRun2_asymptotic_v12_ext4-v1_80000_0007.root\"\n",
    "\n",
    "events = NanoEventsFactory.from_root({ttbar_file: \"Events\"}, mode=\"virtual\", schemaclass=NanoAODSchema).events()\n",
    "\n",
    "# fill a histogram\n",
    "reconstructed_top_mass = calculate_trijet_mass(events)\n",
    "hist_reco_mtop = hist.Hist.new.Reg(16, 0, 375, label=\"$m_{bjj}$\").Double().fill(reconstructed_top_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3d2d1-7b45-47f6-830d-7c46b479f7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize\n",
    "artists = hist_reco_mtop.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998956ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and annotate the visualization\n",
    "fig_dir = Path.cwd() / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "ax.vlines(175, 0, 10000, colors=[\"grey\"], linestyle=\"dotted\")\n",
    "ax.text(180, 150, \"$m_{t} = 175$ GeV\")\n",
    "ax.set_xlim([0, 375])\n",
    "ax.set_ylim([0, 8000])\n",
    "\n",
    "fig.savefig(fig_dir / \"trijet_mass.png\", dpi=300)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb5a13-a8c0-4236-9c71-7ec6847773cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Time for coffea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6684c8",
   "metadata": {},
   "source": [
    "We'll first write the functions to compute the observable and do the histogramming using `awkward` and `hist` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38efe4-8024-422c-ba42-12bd3a3b44cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_TAG_THRESHOLD = 0.5\n",
    "cset = correctionlib.CorrectionSet.from_file(\"corrections.json\")\n",
    "\n",
    "# perform object selection\n",
    "def object_selection(elecs, muons, jets):\n",
    "    electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "    muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                 (muons.pfRelIso04_all < 0.15))\n",
    "    jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "    # Only keep objects that pass our requirements\n",
    "    elecs = elecs[electron_reqs]\n",
    "    muons = muons[muon_reqs]\n",
    "    jets = jets[jet_reqs]\n",
    "\n",
    "    return elecs, muons, jets\n",
    "\n",
    "\n",
    "# event selection for 4j1b and 4j2b\n",
    "def region_selection(elecs, muons, jets):\n",
    "    ######### Store boolean masks with PackedSelection ##########\n",
    "    selections = PackedSelection(dtype='uint64')\n",
    "    # Basic selection criteria\n",
    "    selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "    selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "    selections.add(\"exactly_1b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) == 1)\n",
    "    selections.add(\"atleast_2b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "    # Complex selection criteria\n",
    "    selections.add(\"4j1b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"exactly_1b\"))\n",
    "    selections.add(\"4j2b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_2b\"))\n",
    "\n",
    "    return selections\n",
    "\n",
    "\n",
    "# observable calculation for 4j2b\n",
    "def calculate_m_reco_top(jets):\n",
    "    # reconstruct hadronic top as bjj system with largest pT\n",
    "    trijet = ak.combinations(jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2,\n",
    "                                    np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "    return observable\n",
    "\n",
    "\n",
    "class create_histograms(ProcessorABC):\n",
    "    # create histograms with observables\n",
    "    def process(self, events):\n",
    "        hist_4j1b = (\n",
    "            hist.Hist.new.Reg(11, 110, 550, name=\"HT\", label=r\"$H_T$ [GeV]\")\n",
    "            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "            .Weight()\n",
    "        )\n",
    "    \n",
    "        hist_4j2b = (\n",
    "            hist.Hist.new.Reg(11, 110, 550, name=\"m_reco_top\", label=r\"$m_{bjj}$ [GeV]\")\n",
    "            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "            .Weight()\n",
    "        )\n",
    "    \n",
    "        hist_dict = {\"4j1b\": hist_4j1b, \"4j2b\": hist_4j2b}\n",
    "    \n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "        #process_label = events.metadata[\"process_label\"]  # nicer LaTeX labels\n",
    "    \n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "    \n",
    "        events[\"pt_scale_up\"] = 1.03\n",
    "        events[\"pt_res_up\"] = rand_gauss(events.Jet.pt)\n",
    "    \n",
    "        syst_variations = [\"nominal\"]\n",
    "        jet_kinematic_systs = [\"pt_scale_up\", \"pt_res_up\"]\n",
    "        event_systs = [f\"btag_var_{i}\" for i in range(4)]\n",
    "        if process == \"wjets\":\n",
    "            event_systs.append(\"scale_var\")\n",
    "        \n",
    "        if variation == \"nominal\":\n",
    "            syst_variations.extend(jet_kinematic_systs)\n",
    "            syst_variations.extend(event_systs)\n",
    "        \n",
    "        for syst_var in syst_variations:\n",
    "            elecs = events.Electron\n",
    "            muons = events.Muon\n",
    "            jets = events.Jet\n",
    "    \n",
    "            if syst_var in jet_kinematic_systs:\n",
    "                jets[\"pt\"] = jets.pt * events[syst_var]\n",
    "        \n",
    "            elecs, muons, jets = object_selection(elecs, muons, jets)\n",
    "    \n",
    "            # region selection\n",
    "            selections = region_selection(elecs, muons, jets)\n",
    "    \n",
    "            for region in hist_dict:\n",
    "                selection = selections.all(region)\n",
    "                region_jets = jets[selection]\n",
    "                region_weights = ak.ones_like(ak.num(region_jets, axis=1)) * xsec_weight\n",
    "                if region == \"4j1b\":\n",
    "                    observable = ak.sum(region_jets.pt, axis=-1)\n",
    "                elif region == \"4j2b\":\n",
    "                    observable = calculate_m_reco_top(region_jets)\n",
    "                syst_var_name = f\"{syst_var}\"\n",
    "                if syst_var in event_systs:\n",
    "                    for i_dir, direction in enumerate([\"up\", \"down\"]):\n",
    "                        if syst_var == \"scale_var\":\n",
    "                            wgt_variation = cset[\"event_systematics\"].evaluate(\"scale_var\", direction, region_jets.pt[:, 0])\n",
    "                        elif syst_var.startswith(\"btag_var\"):\n",
    "                            i_jet = int(syst_var.rsplit(\"_\",1)[-1])\n",
    "                            wgt_variation = cset[\"event_systematics\"].evaluate(\"btag_var\", direction, region_jets.pt[:,i_jet])\n",
    "                        syst_var_name = f\"{syst_var}_{direction}\"\n",
    "                        hist_dict[region].fill(\n",
    "                            observable,\n",
    "                            process=process,\n",
    "                            variation=syst_var_name,\n",
    "                            weight=region_weights * wgt_variation,\n",
    "                        )\n",
    "                else:\n",
    "                    if variation != \"nominal\":\n",
    "                        syst_var_name = variation\n",
    "                    hist_dict[region].fill(\n",
    "                        observable,\n",
    "                        process=process,\n",
    "                        variation=syst_var_name,\n",
    "                        weight=region_weights,\n",
    "                    )\n",
    "    \n",
    "        return {events.metadata[\"dataset\"]: hist_dict}\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3816c2",
   "metadata": {},
   "source": [
    "and prepare the fileset we need. More information on how the dataset was prepared can be found [here](https://github.com/iris-hep/analysis-grand-challenge/blob/main/analyses/cms-open-data-ttbar/ttbar_analysis_pipeline.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b151e-d7a8-49d4-8368-310cbe149b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileset preparation\n",
    "N_FILES_MAX_PER_SAMPLE = 4\n",
    "# compared to coffea 0.7: list of file paths becomes list of dicts (path: trename)\n",
    "fileset = utils.file_input.construct_fileset(N_FILES_MAX_PER_SAMPLE)\n",
    "\n",
    "# fileset = {\"ttbar__nominal\": fileset[\"ttbar__nominal\"]}  # to only process nominal ttbar\n",
    "# fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc4d58-c7a7-4e1b-9c3d-ad36af88237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Runner\n",
    "run = Runner(\n",
    "    DaskExecutor(client=client, compression=None),\n",
    "    chunksize=250_000,\n",
    "    skipbadfiles=True,\n",
    "    schema=NanoAODSchema,\n",
    "    savemetrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f40952-2973-44f6-aba1-84d63e9d2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-process\n",
    "samples = run.preprocess(fileset, treename=\"Events\") # treename not needed with coffea master branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15835a57-1182-4efb-8306-07f36af7a5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cloudpickle.register_pickle_by_value(utils) # serialize methods and objects in utils so that they can be accessed within the coffea processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1953d6d",
   "metadata": {},
   "source": [
    "and then we can finally execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dfb17-6806-46c8-aa59-cd475e40cf64",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute\n",
    "tmp, report = run(samples, processor_instance=create_histograms())\n",
    "# sort the key order to be the same as the initial fileset\n",
    "out = {key: tmp[key] for key in fileset}\n",
    "sorted(report[\"columns\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed818b-cb08-46ea-baa3-5cd67e30146c",
   "metadata": {},
   "source": [
    "Notice that the columns being read are the same that `dak.necessary_columns` reported in the `dask-awkward` case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c7fad",
   "metadata": {},
   "source": [
    "To visualize the results, we need to first stack the serperate histograms that were computed individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714929f5-9c56-4afa-87e6-5d096af21ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stack all the histograms together (we processed each sample separately)\n",
    "full_histogram_4j1b = sum([v[\"4j1b\"] for v in out.values()])\n",
    "full_histogram_4j2b = sum([v[\"4j2b\"] for v in out.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2129849-786c-4449-8f48-487e79705c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump for stats inference with also pseudodata\n",
    "for region, histogram in [(\"bin4j1b\", full_histogram_4j1b), (\"bin4j2b\", full_histogram_4j2b)]:\n",
    "    utils.file_output.save_histograms(histogram, f\"all_histograms_fps{N_FILES_MAX_PER_SAMPLE}_{region}.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c0147-6b4e-4ae7-b4e1-b8eb6b764c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j1b[120j::hist.rebin(1), :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, 1 b-tag\");\n",
    "\n",
    "fig.savefig(fig_dir / \"coffea_4j_1b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba9e07-ec3c-4cdb-be16-4cab17e02a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j2b[:, :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, >= 2 b-tags\");\n",
    "\n",
    "fig.savefig(fig_dir / \"coffea_4j_2b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5917353-8085-4e0e-b99e-55d90cb4f170",
   "metadata": {},
   "source": [
    "**Note**: the histogram for $m_{bjj}$ looks slightly different from the one in [the main AGC repo](https://github.com/iris-hep/analysis-grand-challenge/blob/main/analyses/cms-open-data-ttbar/ttbar_analysis_pipeline.ipynb). This is because we are already producing the histogram with the binning used for the statistical inference part, instead of rebenning after producing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bf54a-98bd-4e11-8d38-516ed915a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b-tagging variations\n",
    "ttbar_label = 'ttbar'\n",
    "full_histogram_4j1b[120j::hist.rebin(1), ttbar_label, \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "full_histogram_4j1b[120j::hist.rebin(1), ttbar_label, \"btag_var_0_up\"].plot(label=\"NP 1\", linewidth=2)\n",
    "full_histogram_4j1b[120j::hist.rebin(1), ttbar_label, \"btag_var_1_up\"].plot(label=\"NP 2\", linewidth=2)\n",
    "full_histogram_4j1b[120j::hist.rebin(1), ttbar_label, \"btag_var_2_up\"].plot(label=\"NP 3\", linewidth=2)\n",
    "full_histogram_4j1b[120j::hist.rebin(1), ttbar_label, \"btag_var_3_up\"].plot(label=\"NP 4\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$H_T$ [GeV]\")\n",
    "plt.title(\"b-tagging variations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09d371-f43b-4288-8a6c-a6419235141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jet energy scale variations\n",
    "full_histogram_4j2b[:, ttbar_label, \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "full_histogram_4j2b[:, ttbar_label, \"pt_scale_up\"].plot(label=\"scale up\", linewidth=2)\n",
    "full_histogram_4j2b[:, ttbar_label, \"pt_res_up\"].plot(label=\"resolution up\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$m_{bjj}$ [Gev]\")\n",
    "plt.title(\"Jet energy variations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa67e0-be77-4f70-8a2d-02446ef7793c",
   "metadata": {},
   "source": [
    "This is a plot you can compare to the one in the full AGC notebook â€” you'll notice they look the same. Success!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
